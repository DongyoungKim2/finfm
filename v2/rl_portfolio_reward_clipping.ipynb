{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c897e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import warnings\n",
    "import math\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import vectorbt as vbt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548b603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. 데이터 다운로드 & 전처리\n",
    "# ---------------------------------------------------------------------\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"META\", \"SPY\", \"BND\", \"GLD\"]\n",
    "TRAIN_START, TRAIN_END = \"2010-01-01\", \"2019-06-10\"\n",
    "TEST_START,  TEST_END = \"2019-07-18\", \"2022-07-22\"   # 검증·테스트용\n",
    "\n",
    "\n",
    "def load_prices(tickers, start, end):\n",
    "    px = (\n",
    "        yf.download(tickers, start=start, end=end,\n",
    "                    auto_adjust=False, progress=False)\n",
    "        .loc[:, (\"Adj Close\", slice(None))]\n",
    "    )\n",
    "    # yf 멀티인덱스 (컬럼: 'Adj Close', ticker) → 단순화\n",
    "    px.columns = px.columns.droplevel(0)\n",
    "    return px.sort_index()\n",
    "\n",
    "\n",
    "def pct_returns(prices):\n",
    "    return prices.ffill().pct_change().dropna()\n",
    "\n",
    "\n",
    "prices_all = load_prices(TICKERS, TRAIN_START, TEST_END)\n",
    "returns_all = pct_returns(prices_all)\n",
    "\n",
    "# 학습·테스트 구분\n",
    "train_mask = (returns_all.index >= TRAIN_START) & (\n",
    "    returns_all.index <= TRAIN_END)\n",
    "test_mask = (returns_all.index >= TEST_START) & (returns_all.index <= TEST_END)\n",
    "rets_train, rets_test = returns_all[train_mask], returns_all[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56832298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Gymnasium 포트폴리오 환경\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"On-policy 포트폴리오 환경 (일별 리밸런싱).\"\"\"\n",
    "    metadata = {\"render_modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(self, returns: pd.DataFrame, window: int = 256, fee: float = 0.0005):\n",
    "        super().__init__()\n",
    "        self.returns = returns.values.astype(np.float32)\n",
    "        self.dates = returns.index\n",
    "        self.window = window\n",
    "        self.fee = fee\n",
    "        self.n_assets = returns.shape[1]\n",
    "\n",
    "        # observation: window × n_assets 실수 행렬\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(window, self.n_assets), dtype=np.float32\n",
    "        )\n",
    "        # action: 각 자산별 un-normalized logit\n",
    "        self.action_space = gym.spaces.Box(\n",
    "            low=-5.0, high=5.0, shape=(self.n_assets,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # shape (window, n_assets)\n",
    "        hist = self.returns[self.t - self.window: self.t]\n",
    "        return hist.copy()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.t = self.window    # init position\n",
    "        self.w_prev = np.full(self.n_assets, 1.0 /\n",
    "                              self.n_assets, dtype=np.float32)\n",
    "        return self._get_state(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # softmax → 포트폴리오 weight (합 1.0)\n",
    "        w = torch.softmax(torch.tensor(action), dim=-1).cpu().numpy()\n",
    "        r_t = (w * self.returns[self.t]).sum()\n",
    "\n",
    "        tr_cost = self.fee * np.abs(w - self.w_prev).sum()\n",
    "        reward = r_t - tr_cost\n",
    "\n",
    "        self.w_prev = w.copy()\n",
    "        self.t += 1\n",
    "        done = self.t >= len(self.returns)\n",
    "\n",
    "        return self._get_state(), reward, done, False, {\"date\": str(self.dates[self.t-1]), \"ret\": r_t}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"t={self.t}, weights={self.w_prev}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2be1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Reward-Clipping Callback\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "class RewardClipper(BaseCallback):\n",
    "    \"\"\"Actor-Only PPO에서 어드밴티지 클리핑 (ε₁ < 0, ε₂ = +∞).\"\"\"\n",
    "\n",
    "    def __init__(self, eps_neg: float = -0.4, eps_pos: float = float(\"inf\"), **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.eps_neg, self.eps_pos = eps_neg, eps_pos\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # rollout_buffer → advantages 텐서 in-place clamp\n",
    "        if \"advantages\" in self.locals:\n",
    "            self.locals[\"advantages\"].clamp_(\n",
    "                min=self.eps_neg, max=self.eps_pos)\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f974d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4. 학습\n",
    "# ---------------------------------------------------------------------\n",
    "window = 256\n",
    "env = PortfolioEnv(rets_train, window=window)\n",
    "\n",
    "policy_kwargs = dict(net_arch=[128, 128])\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    n_steps=1024,\n",
    "    batch_size=256,\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.001,\n",
    "    vf_coef=0.0,            # Actor-Only\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    device=device,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "total_timesteps = 2_000_000\n",
    "model.learn(total_timesteps=total_timesteps, callback=RewardClipper(-0.4))\n",
    "model.save(\"ppo_reward_clip_portfolio.zip\")\n",
    "print(\"✅ 모델 학습 완료 & 저장!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa75fbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 5. 테스트 구간 백테스트 (VectorBT)\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def run_policy(model, returns_df, window):\n",
    "    \"\"\"훈련된 모델 → 테스트 가격 → 일별 weight 행렬 생성.\"\"\"\n",
    "    env_test = PortfolioEnv(returns_df, window=window)\n",
    "    obs, _ = env_test.reset()\n",
    "    weights, dates = [], []\n",
    "\n",
    "    while True:\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        w = torch.softmax(torch.tensor(action), dim=-1).cpu().numpy()\n",
    "        weights.append(w)\n",
    "        dates.append(env_test.dates[env_test.t - 1])\n",
    "        obs, _, done, _, _ = env_test.step(action)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    w_df = pd.DataFrame(weights, index=pd.DatetimeIndex(\n",
    "        dates), columns=returns_df.columns)\n",
    "    return w_df\n",
    "\n",
    "\n",
    "w_test = run_policy(model, rets_test, window)\n",
    "\n",
    "# VectorBT 백테스트\n",
    "pf = vbt.Portfolio.from_weights(\n",
    "    price=prices_all.loc[w_test.index, w_test.columns],   # 테스트 구간 가격\n",
    "    weights=w_test,\n",
    "    init_cash=1_000_000,\n",
    "    freq=\"D\",\n",
    "    fees=0.0,\n",
    ")\n",
    "stats = pf.stats()\n",
    "print(\"\\n===== Backtest Stats (Test Period) =====\")\n",
    "print(stats)\n",
    "\n",
    "# 그래프 (주피터/GUI 환경에서 실행 시 표시)\n",
    "try:\n",
    "    pf.plot().show()\n",
    "except Exception as e:\n",
    "    print(\"plot skipped:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
